---
title: Encoding and Evolution
date: 2024-2-12 22:30:00 +/-0
categories: [data-intensive]
tags: [data-intensive, encoding, compatibility] # TAG names should always be lowercase
---

Application들은 user requirement를 더 잘 이해하게 되거나, 뭔가 추가해야하는 등 여러 이유로 변할 수 밖에 없다. 그리고 대부분 app의 feature와 관련된 data에 새로운 field, record type, 기존의 data를 다른 방식으로 표현하는 등의 변경 작업을 동반한다.

RDB에서는 data에 대한 schema migration을 통해 기존 schema를 수정하여 변경 사항들을 반영할 수 있지만 data마다 하나의 schema만 허용하기에 old와 new schema 중 하나만을 강제로 활용해야 한다.
반면에, schema-on-read(“schemaless”) DB들의 경우 schema를 강제하지 않기에 새로운 format의 data와 기존 data format이 같이 공존할 수 있다.

app에서의 변화가 data format의 변화를 가져오듯, data format의 변화 또한 app의 코드 상 수정을 야기하며, 가끔 대규모 app에서는 코드 상에서 이러한 변화를 즉각적으로 반영하기 어렵다:

- server-side app에서는 _rolling upgrade_ (also known as a _staged rollout_) 방식을 통해 몇몇 node에서만 하나씩 new version을 배포하여 잘 작동하는지 확인하며, 점진적으로 모든 node에 이를 배포한다.
  이를 통해 new version이 service downtime 없이 배포될 수 있게하며, 따라서 좀더 빈번한 release와 향상된 evolvabiltiy를 가지게 한다.
- client-side app에서는 new version을 release하여도, user가 update 해주길 바라며 기다릴 수 밖에 없다.

_Backward compatibility_

Newer code can read data that was written by older code.
즉, old and new versions of the code 또는 data formats들이 system에 동시에 잠재적으로 존재할 수 있다는 것을 의미한다. system이 정상적으로 작동하려면, 이러한 compatibility는 양방향으로 유지되어야 한다:

_Forward compatibility_

Older code can read data that was written by newer code.

Backward compatibility은 달성하기 쉽다: 기존 code에서 작성된 data format을 아는 상태에서 새로운 코드추가 작성을 하기에 둘다 명백하게 코드 상에서 보여 쉽게 해결할 수 있다.(예로, old code가 old data 상에 read하는 작업을 계속 수행하게 놔둘 수도 있다.)
Forward compatibility는 기존 code가 new version의 코드에서 추가한 부분을 무시하도록 처리해야 하기에 좀더 달성하기 까다롭다.

이번 장에서는 JSON, XML, Protocol Buffers, Thrift, and Avro 등 다양한 format들의 data encoding 관련 부분을 살피며, 특히 이들이 어떻게 schema 변경을 해결하고 compatibility를 제공하는지 살핀다.
그 다음 이들이 db 그리고 다른 system과의 communication에서 어떻게 사용되는지 알아본다:
web service에서는 communication 방법으로

1. REST(Representational State Transfer)
2. RPC(Remote Procedure Calls)
3. messaging-passing system(actor-queue)
   이 3가지를 활용한다.

# Formats for Encoding Data

프로그램에서는 data를 대부분 두 가지 방식의 표현방법을 통해 사용한다:

1. In memory, data는 objects, structs, lists, arrays, hash tables, trees 등으로 저장되며, data structure들은 효율적인 access와 CPU의 조작(주로 pointer를 활용한)을 위해 최적화 되어있다.
   memory를 공유하는 프로세스간 data 교환에 사용된다.
2. memory를 공유하지 않는 프로세스에 data를 보낼 때는 file 또는 network을 통해 보낸다.
   이를 위해 어떠한 self-contained sequence of bytes 형태로 encode하는 과정이 필요하다.(JSON document와 같이)
   그리고 pointer는 활용될 수 없기에 sequence of bytes 형태는 data structure들과 크게 다른 형태를 띤다.

위 두 방식은 모두 자주 사용되기에 둘 사이에서 서로 변환할 수 있는 방법이 필요하다:

- in-memory 방식 → byte sequence로 변환하는 것을 _encoding_ (also known as _serialization_ or _marshalling_)
- byte sequence → in-memory 방식으로 변환하는 것을 _decoding_ (_parsing_, _deserialization_, _unmarshalling_)
  이라 한다.

## Language-Specific Formats

많은 programming 언어들이 Java has java.io.Serializable와 같이 built-in encoding 기능을 제공한다.
이러한 encoding libraries는 최소한의 code로 in-memory objects들을 저장 및 가져올 수 있도록 하기에 편리하지만, 여러 심각한 문제들을 안고 있다:

- encoing이 특정한 programming 언어와 연과되어 다른 언어로는 encoding된 data를 읽기 어려운 경우가 가끔 있다. 이러한 encoding 방식일 경우 추후 계속해서 특정 programming 언어에 의존성을 가지게되고 system 자체가 다른 programming 언어를 사용하는 system과 integrate되는 것을 불가능하게 한다.
- 같은 object type을 data에서 가져오고자 할 때, decoding 과정에서 임의의 class(Object class와 같이)를 인스터스화 해야한다. 그리고 이 과정은 빈번하게 보안 문제의 원인이 된다: 만약 해커가 app에 임의 byte sequence를 보내어 decode 할 경우, 임의의 class를 인스턴스화 시키기에 원격으로 해커가 작성한 코드를 수행할 수 있어 여러 공격이 가능해진다.
- 가끔 이러한 library들은 data의 versioning에 대해 고려하지 않는다: 빠르고 쉽게 data를 encoding하는 것에만 집중하여, forward와 backward compatibility를 지원하지 않아 매우 불편한 문제를 야기한다.
- Efficiency (CPU time taken to encode or decode, and the size of the encoded
  structure) 또한 이들의 고려 대상이 아닐 수 있다. 예를 들어, Java의 built-in serialization은 최악의 성능과 지나치게 많은 공간과 자원을 소모하는 것으로 유명하다.

이러한 이유들로 보통 언어 자체적으로 지원하는 encoding 기능들을 특별한 목적 없이 사용하는 것은 비추천한다.

## JSON, XML, and Binary

표준화된 encoding 방식에 대해 알아보자.
여러 programming 언어에서는 대표적으로 JSON 그리고 XML이 있으며, 이 둘은 대부분 지원된다.
XML은 verbose하고 불필요하게 복잡하다는 비판을 받는 것에 비해 JSON는 그 간결함과 web browser에서 built-in support로 인해 큰 인기를 받는다. CSV 또한 인기 많은 language-independent format으로 JSON과 XML에 비해 상대적으로 강점이 떨어진다.

JSON, XML 그리고 CSV는 모두 textual format이기에, 사람이 읽을 수 있는 형태이다.
이러한 format은 겉으로 볼 수 있는 syntax 문제 이외에 여러 잠재적 문제가 존재한다:

- Number를 encoding할 때 많은 모호성이 존재한다. XML과 CSV에서는 number type과 string type인 숫자들을 구분할 수 없다.(외부적으로 schema가 존재하지 않을 경우)
  JSON의 경우 number type과 string type을 구별할 수 있지만, precision에 대한 명시가 없기에 integer와 floating-point number을 구별할 수 없다.
  큰 숫자를 다루는데 있어 문제가 존재한다; 예를 들어, IEEE 754 double-precision floating-
  point number 표현 방식에서는 $2^{53}$보다 큰 정수를 나타낼 수 없기에, Javascript와 같이 floating-point number를 표현 방식으로 사용하는 언어의 경우 이러한 숫자를 parsing할 때 부정확성이 발생한다.
  이 때문에 Twitter의 경우 64-bit 숫자를 나타낼 때 API에 JSON number와 decimal string으로 두 번 나타내어 Javascript의 parsing문제를 우회하고 있다.
- JSON과 XML에서는 Unicode 문자 string을 지원 하지만, binary string(sequences of bytes without
  a character encoding)은 지원하지 않는다. Binary string은 유용하기에 보통 Base64를 통해 binary data를 text로 encoding하여 사용하고 schema를 통해 값이 Base64-encoded 되었다는 것을 나타낸다.
  이럴 경우 data 크기가 33%로 증가하지만 작동은 한다.
- XML과 JSON 둘다 optional schema를 지원한다. 이러한 schema 언어는 꽤 강력하고 배우고 적용하기 까다롭다. XML schema는 많이들 사용하지만, 많은 JSON 기반 schema tool들은 아직 잘 사용하지 않는다. schema의 정보에 data의 올바른 해석 여부가 달려 있기에, XML/JSON schema를 사용하지 않는 app은 이 대신 잠재적으로 encoding/decoding logic을 직접 hardcode해야 할 수 있다.
- CSV는 schema가 없으며, app이 어떻게 row와 column을 정의할지에 달려있다.
  만약 app이 새로운 row 또는 column을 추가할 경우, 개발자가 직접 이러한 변화를 반영해야 한다.
  CSV는 또한 꽤 모호한 format(만약 값이 comma 또는 newline 문자를 포함하면 어떡하지?)을 가진다. escaping rule에 대해 공식적으로 명시하였지만 아직 parser들에서 정확히 구현되지 않은 상태다.

미래에 더 효율적인 format이 나오더라도 결국에 가장 중요한 것은 format에 대한 합의이기 때문에 이러한 여러 문제점들에 불구하고 data interchange format에서 JSON, XML 그리고 CSV는 계속해서 많은 목적으로 사용될 것이다.

### **Binary encoding**

data를 내부에서만 사용할 경우 굳이 모두 사용하는 JSON,XML,CSV와 같은 format 말고 좀더 compact하고 빠른 parsing이 가능하지만 외부에서는 잘 사용하지 않는 format을 선택할 수 있다.

JSON은 XML보다 덜 verbose하지만 둘다 binary format과 비교하면 많은 공간을 차지하기에, JSON/XML을 binary encoding하는 여러 format들이 개발되었다.
이러한 format들은 적재적소에 잘 사용되지만 전체적으로 봤을 땐 아직 JSON 및 XML의 textual version에 비해서 사용되는 정도가 미미하다.

JSON/XML binary encoding하는 format에서 몇몇의 경우 기존의 datatype들을 수정(integer와 floating-point number를 구분, 또는 binary string을 추가)하지만, 이외의 것들은 모두 기존 data model을 그대로 유지한다. 여기서 특별히 볼 점은 이들은 schema를 이용하지 않기에 encoded data에 object field name을 포함하여야 한다.
즉, JSON을 binary encoding을 진행할 경우 Example 4-1에 있는 userName, favoriteNumber 그리고 interests에 대한 string들을 어딘가에 포함하여야 한다는 것이다.

_Example 4-1. Example record which we will encode in several binary formats in this chapter_

```jsx
{
	"userName": "Martin",
	"favoriteNumber": 1337,
	"interests": ["daydreaming", "hacking"]
}
```

JSON을 binary encoding하는 MessagePack의 결과를 Figure 4-1로 살펴보자.

![Desktop View](../../assets/data_intensive_apps/4%20Encoding%20and%20Evolution%202f42cf3225994e408980312dc40320e6/fig4-1.png)

1. 첫 byte인 0x83에서 0x80은 object를 0x03은 3개의 field가 존재한다는 것을 의미한다.
   만약 field의 수가 15개가 넘어 숫자를 4bits에서 담아내지 못한다면 다른 indicator type을 통해 field의 수를 나타내는 byte가 2 또는 4byte로 설정된다.
2. 두번째 byte, 0xa8에서 0xa0은 string type을 0x08은 8byte의 길이를 가진다는 것을 나타낸다.
3. 다음 8byte는 field name인 userName을 ASCII format으로 나타낸다. 두 번째 byte에서 string의 length를 나타내기에 별도의 escape char나 string end를 나타내는 char을 포함하지 않는다.
4. 다음 7byte는 위와 마찬가지로 prefix 0xa6에서는 string type과 length 그리고 나머지에서는 string에 대한 문자들을 표현한다.

결과적으로 총 66byte의 크기로 encoded 되었으며, 이는 JSON에서 whitespace를 제거한 textual encoded의 결과인 81byte의 크기로 별반 차이가 없다. 이러한 작은 크기의 공간을 아끼기 위해 human-readable을 포기하는 것이 맞나 싶겠지만, 다음 차례에서는 같은 JSON을 32byte로 줄이는 방법을 살펴보자.

## Thrift and Protocol Buffers

Thrift와 Protocol Buffers 모두 binary encoding을 위한 library로 이전과 같은 원칙을 기반으로 한다.

둘다 data encoding을 위해 schema를 필요로 하며, 그 형태는 아래와 같다.

_Thrift interface definition language(IDL):_

```jsx
struct Person {
1: required string userName,
2: optional i64 favoriteNumber,
3: optional list<string> interests
}
```

_Protocol Buffers:_

```jsx
message Person {
        required string user_name       = 1;
        optional int64  favorite_number = 2;
        repeated string interests       = 3;
}
```

둘다 code generation tool을 지원하여 schema definition을 통해 class를 생성해 다양한 언어에서 schema를 구현해준다. 그리고 app에서는 이렇게 생성된 code를 호출하여 encode 또는 decode를 수행할 수 있다.

Thrift에는 두 가지 encoding format인 *BinaryProtocol*과 *CompactProtocol*이 존재한다.
Figure 4-2. _Thrift’s_ _BinaryProtocol_

![Desktop View](../../assets/data_intensive_apps/4%20Encoding%20and%20Evolution%202f42cf3225994e408980312dc40320e6/fig4-2.png)

총 59byte를 차지한다.
Figure 4-1과 비슷하게, 각 field의 type에 대한 annotation(stirng, list, int 등)이 있고 string을 ASCII로 encoding을 한다.

Fig 4-1과의 차이점은 field name을 직접적으로 저장하지 않고 이 대신 field tag를 값으로 가지고 있다. field tag는 field name의 aliases라 볼 수 있으며, 좀더 compact하게 어떤 field를 나타내는지 알 수 있다.

Figure 4-3. _Thrift’s_ _CompactProtocol_

![Desktop View](../../assets/data_intensive_apps/4%20Encoding%20and%20Evolution%202f42cf3225994e408980312dc40320e6/fig4-3.png)

총 34byte를 차지한다.
이전과 다르게 field tag와 type annotation을 하나의 byte로 합쳐서 나타내었으며, variable length integer를 통해 int 1337을 int를 위해 할당된 8byte 전체를 이용하지 않고 그 크기에 맞게 2byte로 줄여 나타내었다.
variable length integer에서는 맨 앞 byte에서는 뒤에 따라오는 byte가 있는지 없는지 나타낸다.
따라서 -64 ~ 63은 1 byte로, -8192 ~ 8191은 2byte로 encoding이 된다. 즉, 숫자가 크면 더 많은 byte를 이용하고 작으면 더 적은 byte를 사용한다.

Fig 4-4. _Protocol Buffers_

![Desktop View](../../assets/data_intensive_apps/4%20Encoding%20and%20Evolution%202f42cf3225994e408980312dc40320e6/fig4-4.png)

총 33byte를 차지한다.
CompactProtocol과 bit packing 방식만 살짝 다를 뿐 이외는 CompactProtocol과 많이 유사하다.

알면 좋은 점: 위에서 본 schema에서 required 또는 optional이 명시되었는데 이는 단순히 runtime 때 check하여 해당 field가 존재하면 pass 없으면 fail하는 형태로 작동하지 encoding에 어떠한 영향을 주지 않는다. 그래서 버그를 찾아낼 때 유용하다.

### **Field tags and schema evolution**

schema는 시간에 따라 변경될 수 밖에 없고 우린 이를 *schema evolution*이라 부른다. 그럼 Thrift와 Protocol Buffers에서는 어떻게 이러한 변화를 Forward/Backward compatibility를 유지하면서 처리하는지 알아보자.

각 field들은 tag number를 식별자로 사용하고, type에 대한 annotation이 존재한다.
따라서 만약 특정 field값이 존재하지 않는다면, 단순히 encoded data 내에 포함하지 않고 진행하면 된다.
field tag는 schema에 정의된 field과 match되기에 어떤 field인지 나타내는데 매우 중요한 역할을 한다. 이는 직접적인 name을 사용하지 않기에 schema에서 field name은 바꿔도 아무런 문제가 없다. 하지만 만약 field tag를 바꾼다면 encoded된 data들을 모두 invalid하게 만들어 버린다.

만약 새로운 field를 추가한다면, 기존에 사용된 field tag와 겹치지 않은 번호를 부여하고 schema에 추가만 하면 된다. old code에서 new data를 볼 경우(forward compatibility), 단순히 추가된 field를 명시된 type annotation을 이용해 그 length만큼 무시하고 다음 field로 진행하여 read를 수행할 수 있다.

backward compatibility의 경우 new code에서 old data를 read할 때 tag number들만 고유한 식별자로 유지된다면 문제될 점이 없다. 단지, 새로 추가된 field를 required로 설정하지 말아야한다는 것을 유의하자.
만약 required인 field가 추가된다면 old data들에는 해당 field가 없어 fail이 발생할 것이기 때문이다.
따라서 추가할 field는 항상 optional 또는 default value가 존재하도록 한다.

field를 제거하는 경우도 추가하는 경우와 똑같다. 다만, add의 경우에서 고려한 backward와 forward 입장이 서로 반대될 뿐이다. 따라서 field를 제거할 경우 항상 optional인 것만 가능하며 이후 제거된 field의 tag number는 사용하지 않는다.(혹시 해당 tag number를 가진 data가 아직 존재하여 new code에서 read 될 수 있기 때문에)

### **Datatypes and schema evolution**

schema에서 field의 datatype을 바꿀 경우는 어떨까?

저장된 value의 precision을 잃거나 일부가 잘릴 가능성이 존재한다. 만약 32-bit integer를 64bit로 바꾼다면 빈 공간을 0으로 채우기 때문에 문제가 없지만 반대인 경우 절반의 값이 잘려나가게 된다.

Protocol Buffers의 경우 type에 list 또는 array가 아닌 repeated가 존재하는데 이는 같은 type의 field가 반복된다는 의미이다. 따라서 optional field에 한해 single-valued type을 list type으로 바꿀 수 있으며, old data는 list가 비었거나 하나의 element가 있는 것으로 인식한다. old code에서는 new data를 읽을 때 list의 마지막 element만 인식한다.

Thrift의 경우 list type이 존재하며, list 내 element type은 parameterized되기에 중첩 list가 가능하다.
하지만 이 경우 Protocol Buffers와 같이 single-valued type을 multi-valued로 바꿀 수는 없다.

## Avro

Hadoop의 use case에 맞게 사용하기 위해 Apache에서 개발한 또 다른 binary encoding format이다.

Avro에서도 schema를 사용하여 encoding되는 data의 구조를 나타내며, 두 가지 schema language가 존재한다: 하나는 human editing을 위한 Avro IDL이며, 또 다른 하나는 JSON을 기반으로하여 machine-readable한 형태이다.

Schema는 아래와 같다.

1. _Avro IDL_

```jsx
record Person {
string userName;
union { null, long } favoriteNumber = null;
array<string> interests;
}
```

1. _Avro’s JSON representation of same schema_

```jsx
{
        "type": "record",
        "name": "Person",
        "fields": [
{"name": "userName", "type": "string"},
{"name": "favoriteNumber", "type": ["null", "long"], "default": null},
{"name": "interests", "type": {"type": "array", "items": "string"}}
] }
```

Figure 4-5. _Example record encoded using Avro_

![Desktop View](../../assets/data_intensive_apps/4%20Encoding%20and%20Evolution%202f42cf3225994e408980312dc40320e6/fig4-5.png)

Avro의 경우 encode한 결과 중 가장 compact한 32byte의 크기를 가진다.
가장 눈에 띄는 점은, byte sequence에서 type과 field tag number가 없다는 점이다.
string의 경우 UTF-8로 encode되며, length prefix만을 가지고 integer와 같이 나머지도 별도의 type에 대한 명시가 없다.
Integer는 Thrift의 CompactProtocol과 같이 variable-length encoding을 가진다.

Avro에서는 encoded된 data를 parsing하기 위해서는 schema를 통해 field type을 확인한다. 따라서 data는 encoding에 사용된 exact same schema를 통해서만 정확히 decoded 될 수 있다.
만약 reader와 writer 간 mismatch가 schema에 존재한다면 잘못된 decoding이 수행될 것이다.

### **The writer’s schema and the reader’s schema**

그러면 Avro에서는 schema evolution을 어떻게 달성할까?

app에서 data를 encoding(DB 또는 file에 write, network를 통해 송신)을 할 때 app에 compiled된 schema를 활용하며, 이를 *writer’s schema*라 한다.
만약 반대로 app에서 data를 decoding(read from DB or file, network를 통해 수신)을 할 때 app을 build하는 시점에서 schema로부터 생성된 code를 이용하며, 이를 *reader’s schema*라 한다.

**가장 핵심은 Avro에서는 reader와 writer가 같은 schema를 사용하지 않아도 되며, 그저 compatible 하면 된다는 점이다.**
data가 decoded(read) 될 때, Avro의 library에서는 writer’s schema와 reader’s schema를 두고 다른 점을 찾고 data를 writer’s schema에서 reader’s schema로 변환한다.

Figure 4-6. _Avro reader and writer_

![Desktop View](../../assets/data_intensive_apps/4%20Encoding%20and%20Evolution%202f42cf3225994e408980312dc40320e6/fig4-6.png)

예를 들어 두 schema간 field가 정의된 순서가 달라도 field name을 통해 이를 알아서 matching해준다.
reader에는 존재하지만 writer의 schema에는 없다면 이를 무시하고, reader에서 특정 field에 대한 값을 기대하는데 writer의 schema에 해당 field name이 보이지 않는다면 deafult 값을 선언하여 가져온다.

### Schema evolution rules

Avro에서 forward compatibility는 new version writer’s schema와 old version reader’s schema를 의미하며, backward compatibility는 new version reader’s schema와 old version writer’s schema이다.

compatibility 유지를 위해 항상 field를 추가하거나 제거할 때 default value가 설정되어 있는 field에 대해서만 작업한다.(Figure에서 favoriteNumber field를 보면 default value of null을 가지고 있다)
따라서, old writer schema로 작성된 data를 new reader schema에서 읽을 때 추가한 field에 대한 값이 없으면 default value를 넣어 처리할 수 있다.

만약 default value 없는 field가 추가된다면, new reader에서는 old data를 처리할 수 없을 것이며 결과적으로 backward compatibiliy가 깨진다. 또한 같은 field에 대해 제거한다면 old reader에서는 new data에서 보이지 않는 field를 처리하지 못할 것이다.

다른 여러 programming 언어에서는 null 값을 변수에 대한 default 값으로 설정할 수 있지만, Avro의 경우는 아니다. 따라서 이를 위해 *union type*을 이용하여 여러 type 중 null type이 가능하다는 방식으로 나타낸다.
예를 들어, union { null, long, string } field;는 해당 field가 3가지 type 중 하나가 될 수 있다는 식이다.
이러한 방식은 좀 verbose하지만 bug를 찾고 예방하는데 유리하다.

결과적으로, Avro는 required나 optional marker를 가지고 있지 않다.

Avro에서도 기존 field에 대한 type을 바꿀 수 있지만 좀더 까다롭다:
reader’s schema에서는 field name에 대한 aliases를 가질 수 있으며 이를 통해 writer’s schema에 있는 field name을 match할 수 있다. 따라서 backward compatible 하지만 not forward compatible.

### But what is the writer’s schema?

궁금한 점은 reader는 어떻게 특정 data가 encoded할 때 사용한 writer’s schema에 대해 알아낼까? 이다.
모든 encoded data에 사용한 schema에 대한 모든 내용을 넣을 경우 크기를 줄이기 위해 binary encoding하는 이유가 없기 때문에 다른 방식이 필요하다.

답은 Avro가 사용되는 context에 따라 다르다. 몇 가지 예를 들어보자.

_Large file with lots of records_

Hadoop에서 사용할 일이 많기에 Avro에서 흔한 use로 몇 백만 record들을 저장한 large file을 모두 같은 schema로 encoding하는 경우이다. 이러한 경우, writer’s schema를 file 시작 부분에 포함한다. 이를 위해 Avro에서는 file format(object container files)을 명시한다.

_Database with individually written records_

여러 다른 record들이 다른 시점에서 다른 writer’s schema를 이용해 저장된 case이다.
이러한 경우 record들이 같은 schema로 작성되었다는 보장이 없기에 각 record 시작 부분에 사용된 schema의 version을 저장하는 것이다. 그리고 db에다 schema list를 저장하여 version 관리를 한다.
그러면 reader에서는 db에서 version에 해당하는 schema를 찾아 fetch하고 이를 이용해 decode한다.

_Sending records over a network connection_

양방향 network 연결로 두 프로세스 간 송수신을 할 때, 처음 connection setup에서 schema version에 대한 합의를 이뤄낸다. 이러한 합의는 connection이 살아있는 동안 지속된다. Avro RPC protocol이 이 방식으로 동작한다.

schema version들에 대한 db는 document 역할과 compatibility를 확인할 수 있게 하기에 모든 case에서 활용하기 좋다. version을 명시하는 방법으로는 간단히 integer를 increment하는 방식과 hash를 활용할 수 있다.

### Dynamically generated schemas

Avro의 방식은 tag number가 없기 때문에 _dynamically generated_ schemas에 대한 장점을 가진다.

예로, RDB에 저장된 모든 record들을 textual format이 가진 문제를 피해 binary format으로 file에다 쓰고 싶은 경우 Avro에서 제공하는 tool을 통해 각 table은 schema로, 그 안의 column들은 field로 name을 mapping하여 생성할 수 있다.

그리고 db schema가 변할 경우(column 추가 또는 제거), 새로운 Avro schema를 updated된 db의 schema를 기반으로 재생성 후 data를 export하면 된다. 이 때 data export 과정에서 schema conversion을 매번 수행하기에 따로 신경 쓸 필요도 없다.

반면에 만약 Thrift나 Protocol Buffers를 이용하는 경우, field tag는 보통 직접 할당되기 때문에 매번 db schema가 변하면 직접 column의 변경사항을 field tag에 mapping하는 작업을 수행해야 한다.(이를 자동화 할 수 있지만, schema generator가 이전에 사용되었던 tag number를 사용하지 못하도록 신경 써야한다.)

이러한 dynamically generated schema는 Thrift나 Protocol Buffer의 design goal이 아니지만 Avro에서는 이를 고려해 design 되었다.

### **Code generation and dynamically typed languages**

Thrift나 Protocol Buffers의 경우 code generation에 의존한다: schema가 정의되면 해당 내용을 구현하는 class를 code 상에서 구현한다. 이는 보통 statically typed 언어인 Java, C++에서 유용한데, 이는 code generation을 통해 decoded data를 위해 효율적인 in-memory structure를 사용할 수 있게 하며, type checking 및 autocompletion을 IDE에서 제공할 수 있기 때문이다.

dynamically typed 언어인 Javascript, Python 등의 경우, compile time이 없기에 이러한 장점을 누리지 못한다. 따라서 이 경우 code generation은 불필요하며 dynamically generated schema의 경우에도 code generation은 data를 가져오는데 불필요한 절차이다.

Avro에서는 statically typed 언어를 위해 optional code generation을 제공하지만, 여기서도 이것 없이 잘 사용 가능하다.
예를 들어, 만약 object container file(which embeds the writer’s schema)이 있으면, Avro library를 사용해 파일을 열어 JSON file을 보듯 data들을 확인할 수 있다. 이는 파일에 필요한 모든 metadata가 저장되어 있기에 가능하다.

이러한 특성은 dynamically typed data processing 언어인 Apache Pig 등에서 특히 유용하며, 이러한 언어에서는 schema를 고려할 필요 없이 Avro file을 열어 분석하고 여기서 파생된 dataset을 Avro format으로 바로 작성 가능하다.

## The Merits of Schemas

지금까지 본 binary encoding format에서 사용하는 schema들은 XML 또는 JSON schema보다 덜 detail하지만 구현 및 활용이 더 쉽다.

이러한 아이디어들은 ASN.1이라는 schema definition language에서 처음 나왔지만 이를 사용하기에는 document가 별로 없고 매우 복잡하다.

많은 data system에서는 자체적인 binary encoding을 활용하고 있으며, 대부분의 vendor들은 이를 위한 driver를 제공하여 db에서 오는 response를 network protocol에서 in-memory data structure로 decode 할 수 있게 해준다.

Textual data format인 XML, JSON 그리고 CSV가 현재까지 가장 많이 사용되고 있지만, schema를 기반으로한 binary encoding이 가지는 장점들에 대해 다시 언급해보면:

- field name을 직접 data에 encode하지 않기에 다양한 “binary JSON”보다 훨씬 compact하다.
- schema가 있기에 document로 사용할 수 있고 이를 활용해 decode하기에 항상 update가 반영된다.
  반면에 직접 documentation을 작성할 경우 이에 대한 반영이 늦어질 수 있다.
- schema들에 대한 db를 통해 schema update 시 compatibility를 확인할 수 있다.
- statically type 언어 사용자들을 위해, schema로부터 code를 generate해주고 이를 통해 compile time에 type checking이 가능하다.

요약하자면, schema evolution은 schemaless/schema-on-read JSON db보다 더 나은 data에 대한 보장을 제공하면서 같은 수준의 flexibility를 이룰 수 있게 해준다.

# Modes of Dataflow

memory를 공유하지 않는 process에 data를 보내고 싶을 때는 data를 sequence of byte로 encoding하는 과정이 필요하며, 지금까지 이러한 여러 encoding 방식들을 살펴보았다.

Compatibility를 통해 시스템의 evolvability(한번에 모든 변화들을 크게 반영해야 하는 것이 아닌, 시스템 요소 각각 독립적으로 변화를 반영할 수 있다.)를 논의하였다.
결국 Compatibility는 encoding하는 프로세스와 decoding하는 프로세스 사이의 관계를 의미한다.

이번 section에서는 이러한 관계에 대해 더 자세히 알아보기 위해 가장 흔한 3가지 dataflow를 살펴보자.

- Via database
- Via service calls
- Via asynchronous message passing

## Dataflow Through Databases

Database의 경우 프로세스만 접근하여 db에 write할 때 data를 encode하며, db에서 read할 때 decode를 진행한다.
이러한 프로세스가 하나인 경우에는 그 later version of 프로세스가 이전에 자신이 write했던 data를 그대로 decode할 것이기에 이를 _sending a message to your future self라_ 하기도 한다.

보통의 경우 여러 다른 프로세스들이 db에 접근하며, 각자 new code를 실행하거나 old code를 실행할 수 있다.

따라서 위 두 경우 모두 backward와 forward compatibility가 요구된다.

추가적으로 고려할 상황은 new code에서 작성한 data를 old code에서 read(data를 model object로 decode)하여 이를 update한 후 write(model object를 encode)할 때, 변경된 field에 대해 read 시 ignore 하지만 write 시 이를 기억하여 변경된 field에 대한 정보를 유지해야 한다는 점이다.

_Figure 4-7. When an older version of the application updates data previously written
by a newer version of the application, data may be lost if you’re not careful._

![Desktop View](../../assets/data_intensive_apps/4%20Encoding%20and%20Evolution%202f42cf3225994e408980312dc40320e6/fig4-7.png)

### **Different values written at different times**

application code의 update를 시스템 전체에 반영하는데에는 몇 분의 시간만 소요되지만, db에 저장된 data의 경우 직접 전부 rewrite(migrate)하지 않는한 예전의 data들이 그대로 존재한다. 그리고 이러한 현상을 *data outlives code*라 \*\*부른다.

data를 rewriting(migrating)하는 것은 분명 가능하지만 db에 저장된 large dataset들에 대해 이를 수행하려면 많은 시간과 자원이 소모된다. 따라서 많은 RDB에서는 간단히 schema change를 통해 old data에 새로 추가된 column이 없다면 이를 null 값으로 넣어 encoding하여 해결한다.
LinkedIn의 document DB인 Espresso에서는 Avro의 schema evolution rules를 이용해 이를 수행한다.

결과적으로 schema evolution을 통해 실제로는 여러 historical version들의 schema를 통해 encoded된 data들이 전부 하나의 schema를 통해 encoded 되었던 것처럼 나타낼 수 있는 것이다.

### **Archival storage**

때론 특정 시간 마다 DB에 있는 data들에 대한 snapshot을 백업 또는 data warehouse에 load할 목적으로 생성할 수 있다.
이 경우 여러 historical version들의 schema를 통해 encoded된 data들이 전부 하나의 schema를 통해 encoding 한다. 어차피 data들을 전부 copy할 때 consistency를 유지하면 좋기 때문이다.

이러한 data dump는 한번에 수행되며 이후 생성된 file은 immutable하다.
이 경우 Avro의 object container file을 사용하기 좋으며, analytics-friendly column-oriented format으로 encoding하여 크기를 줄이기 좋다.

## Dataflow Through Services: REST and RPC

프로세스간 network를 통해 통신을 수립하는 방법은 여러가지가 있으며, 가장 대표적인 경우는 client and server간 통신 수립이다. server에서는 API를 network 상에 노출하고, client는 해당 API에 request를 보낸다. 그리고 이러한 API를 우린 *service*라 한다.

service와 db는 client가 data를 query 할 수 있게 한다는 부분에서 유사한 점을 가지고 있다.
하지만 db의 경우 어떤 임의의 query라도 허용하지만 service의 경우 API를 통해 encapsulation을 가진다. 따라서 API에서 client의 허용 범위에 대해 fine-grained restriction을 가할 수 있다.

client가 web browser인 경우 보통 HTTP protocol을 통해 API에 request를 보내고 이에 대한 response로 표준인 data format인 HTML, CSS, JavaScript, static files 등을 받는다.
현재 web browser 이외에도 mobile app, desktop 등 많은 device들이 API에 HTTP request를 보내는데 이러한 경우에는 HTML 대신 JSON과 같이 client-side에서 처리하기 쉬운 format으로 response를 보낸다.

server가 다른 service에 대한 client가 될 수 있는데 가장 대표적인 경우는 _service-oriented architecture(SOA)_ 또는 *microservices architecture*이다.
대규모 app을 service 기능 단위로 나누고 각자 update, 유지보수, 배포 등을 독립적으로 진행할 수 있게하는 최종 목적은 **시스템의 evolvability 증대**이다. 따라서 여러 version들의 code가 공존하여 수행될 수 있기에 compatibility가 중요해진다.

### Web services

HTTP protocol이 service와 통신할 때 사용된다면 이는 모두 *web service*이다.
명칭이 살짝 애매한데 이는 web뿐 아니라 다른 여러 context에서도 web service가 사용되기 때문이다. 예를 들어:

1. mobile native app 또는 Javascript web app using Ajax(XML)들이 HTTP request를 인터넷을 통해 보내는 case
2. microservices architecture로 구성되었다면, 같은 조직 또는 같은 datacenter 내 service들간 request를 보내는 case(이러한 use case를 support 해주는 software를 *middleware*라 한다.)
3. 어떤 service가 다른 외부 조직에 있는 service를 인터넷을 통해 request를 보내는 case
   이를 통해 다른 조직과 data exchange를 하며, 대표적으로 OAuth, credit card processing system과 같은 public APIs가 있다.

web service에서 가장 많이 사용되는 방법은 두 가지가 있다: _REST_ and _SOAP_

*REST*는 protocol이 아닌 design \**philosophy로 HTTP를 기반으로 한다.
원칙으로는 simple data format, URLs을 활용한 자원 표현 그리고 HTTP 특성을 활용한 caching, authentication, 그리고 content type negotiation 등이 있다.
cross-organizational service integration이나 microservices에서 자주 사용되며, 이러한 REST원칙을 따르는 API들을 *RESTful\*이라 한다.

반면, SOAP는 XML기반 통신 protocol이다.
HTTP를 기반으로 많이 사용되지만, HTTP로부터 독립을 목표로 여러 HTTP 기능들을 사용하지 않으려 한다. 대신, 넓고 복잡하게 얽혀있는 관련 표준인 WS-\*들로 불리는 framework들을 활용한다.
SOAP web service의 API는 XML기반 언어인 WSDL(Web Services Description Language)를 사용해 나타내진다. 그리고 WSDL을 통해 code generation을 하여 client가 local class 또는 method 호출을 통해 service들에 접근할 수 있다.
WSDL은 not human-readable하고 SOAP message는 너무 복잡하여 관련 tool 및 code generation에 의존해야 한다. 그리고 여러 vendor들간 구현 방식에 차이로 interoperability(호환성)가 떨어지기에 현재 점점 사용이 줄고 있다.

### T**he problems with remote procedure calls (RPCs)**

web service는 API에 request를 보내는 많은 기술들 중 하나이며, 이들 모두 RPC를 기반으로 한다.
RPC model에서는 같은 프로세스 내에서 network에 request를 보내는 것을 함수를 호출하는 것과 같이 보이게 만들려고 한다.(이러한 추상화를 *location transparency*라 한다.)
이러한 RPC들은 처음에는 편리해보였지만, 특성이 너무나 다른 둘 이기에 근본적으로 잘못된 방식이다.

- local function call은 예측 가능하며, succeeds or fail 이 두가지 뿐이다.
  하지만 network request의 경우 service에 request가 안 간건지, response가 오지 않는 건지 아니면 service 자체가 unavailable한지 등 결과에 대한 원인을 정확히 알기 어렵기에 예측할 수 없다.
- local function call의 결과는 return, no return(infinite loop/crash 이유로), exception 이 3가지로 볼 수 있다. network request에서는 timeout으로 인해 결과 없이 return 되는 경우 등 결과에 대한 가능한 원인이 많아 파악하기 어렵다.
- 만약 response가 오지않아 request가 failed할 때 client에서 request를 여러 번 보내고 이 때문에 같은 action을 여러 번 수행하기에 deduplication protocol(_idempotence_)에 대한 처리 방식을 고려해야한다.
- request의 latency는 network congestion이나 service가 과부하된 상태 등 여러 이유가 존재하며, 이 때문에 결과를 받는데 걸리는 시간이 자주 달라진다.
- request에서는 param을 encoding하여 보내줘야 하기에 primitive type인 경우 괜찮지만 큰 object들의 경우 문제가 될 수 있다.(한번 request에 담을 수 있는 크기는 제한되어 있기 때문)
- client와 server간 서로 다른 programming 언어를 사용할 경우 RPC framework에서 각 언어의 datatype에 맞게 변환해야 한다. 이 경우 Javascript에서 $2^{53}$보다 숫자가 큰 경우 문제가 생기는 것과 같이 언어마다 같은 type이 있지 않기에 문제가 발생할 수 있다.

위와 같은 이유로 차라리 network protocol인 것을 코드에서 명확히 나타내는 것이 local object처럼 만들어 보이는 것보다 훨씬 낫다.

### Current directions for RPC

gRPC의 경우 Protocol Buffers를 사용한 RPC이며, 이외 여러 RPC들이 사용되고 있다.

이러한 새로운 RPC framework들은 remote request이지 local function call이 아니라는 부분을 더 명확히 들어내고 있다. 예로 javascript promise 등을 통해 asynchronous action을 처리하거나 gRPC의 streams처럼 series of request와 response들로 구성된 remote service 호출 방식이 있다.

몇몇 framework에서는 _service discovery_ 기능을 통해 client가 어느 IP와 port 번호를 활용해 특정 service를 사용할 수 있는지 알 수 있게 제공한다.

Custom RPC protocol을 통해 binary encoding을 하여 성능을 더 높일 수 있지만 RESTful API가 활용도, 디버깅, 대부분 언어와 platform의 지원을 통해 구성된 ecosystem 등을 이유로 제일 많이 사용된다.

따라서 현재 public APIs들은 전부 REST로 구현되며, RPC의 경우 내부 service들간 통신에 초점을 잡아 활용되고 있다.

### **Data encoding and evolution for RPC**

service에서는 db와 다르게 간단한 전제를 깔고 얘기할 수 있다: 항상 server가 먼저 update되고 client가 나중에 된다.
따라서 request는 backward compatibility만 response는 forward compatibility만 지원하면 된다.

RPC에서 compatibility는 이들이 사용하는 encoding 방식의 영향을 받는다:

- Thrift,gRPC(Protocol Buffers) 그리고 Avro RPC는 각각의 encoding format에 compatibility rule을 따라 evolve한다.
- SOAP에서는, XML schmea를 통해 req와 res가 명시되며 evolve한다.
- RESTful API들은 res로 JSON(별도의 schema 없이)을 가장 많이 사용하며, req param으로는 URI-encoded/form-encoded를 많이 사용한다. req에 parameter를 추가하거나 res에 field를 추가하는 경우를 evolve하는 것이라 볼 수 있다.

service compatibility에서는 client에게 update를 강제할 수 없기에 오랫동안 유지되어야 하며, 보통 compatibility가 깨질만한 update의 경우 API version을 아예 나눠 관리한다.
이러한 API version들은 URL이나 HTTP Accept header를 사용해 나타내거나 client의 API key를 통해 식별할 수 있다.

## Message-Passing Dataflow

RPC와 DB의 사이에 존재한다.
RPC와 같이 sender에서 request(_message_)를 다른 프로세스에 보내며, direct connection이 아닌 중간에 message broker(_message queue_)를 두어 message를 임시적으로 저장한 후 receiver가 처리할 여유가 있을 때 이를 pulling하여 수행하는 방식이다.

message queue 이용 시 direct RPC와 비교할 때 얻는 이점들은:

- receiver가 과부하 상태이거나 unavailable할 때 buffer 역할을 통해 reliability를 높힌다.
- process가 crash하는 경우 redeliver를 통해 message가 소실되지 않게 해준다.
- Cloud의 경우 VM의 IP와 port가 계속해서 바뀔 수 있는데, message queue를 통해 message를 전달하기에 이에 대한 고민을 하지 않아도 된다.
- 하나의 message가 여러 recipients에게 전달될 수 있다.
- sender와 recipient의 decoupling이 가능하다.

RPC에서는 request를 보내면 이에 대한 response를 기대하지만, message-passing의 경우 one-way communication이다. 따라서 sender는 message를 보낸 후 바로 다음 작업을 수행한다.
만약 response를 보내는 경우 별도의 channel을 형성하여 이를 보낸다.

### Message brokers

이전에는 기업들이 소유한 software가 많았지만 현재 open source인 RabbitMQ, ActiveMQ, HornetQ, NATS, and Apache Kafka 등이 자주 사용된다.
구현 방식과 configuration에서는 각자 차이가 있지만 일반적으로 사용되는 방식은 아래와 같다:
한 프로세스에서는 message를 named queue or topic에 보내면, broker는 message가 해당 queue의 consumer나 topic의 subscriber들에게 전달될 것을 보장한다.
하나의 queue/topic에 여러 producer와 consumer가 존재할 수 있다.

Message broker에서는 data model을 강제하지 않으며- message는 단지 sequence of bytes with some meatdata이다. 따라서 어떠한 encoding format도 사용 가능하며, 만약 사용하는 encoding이 backward and forward compatible 하다면 producer와 consumer들을 독립적으로 변화시키고 배포할 수 있는 최대한의 flexibility를 얻을 수 있다.

consumer가 message를 받아 또 다른 topic에 republish하는 chaining의 경우 Fig 4-7에서 언급했던 것처럼 unknown field을 보존해야한다는 점에 유의하자.

### Distributed actor frameworks

*actor model*은 하나의 프로세스에서 concurrency를 위한 programming model이다.
race condition, deadlock 등의 문제를 가지는 Thread 대신 logic들을 _actor_ 단위로 encapsulate하는 것이다.

각 actor는 client 또는 entity를 나타내며 각자 자신들만의 여러 state들(cannot be shared with others)을 가질 수 있다. 그리고 actor들간 asynchronous messages를 주고받으며 작동한다.
Message delivery는 보장되지 않는다: 특정 error scenario에서는 message가 소실될 수 있으며, 각 actor마다 한번에 하나의 message만 처리하기에 thread에 대해 생각할 필요가 없다.

*distributed actor frameworks*는 application을 여러 node를 통해 scaling하는 프로그래밍 model이다.
sender 그리고 recipient가 같은 노드 또는 서로 다른 노드에 있든 message-passing mechanism을 통해 통신하며, 다른 노드에 있는 경우 byte sequence로 encoding하여 network를 통해 통신한다.
따라서 기본적으로 message를 이용하기에 local과 remote 통신 사이에 mismatch가 더 적어 location transparency가 RPC에서보다 더 잘 작동한다.

distributed actor framework essentially integrates a message broker and the actor programming model into a single framework. However, if you want to perform roll‐ ing upgrades of your actor-based application, you still have to worry about forward and backward compatibility, as messages may be sent from a node running the new version to a node running the old version, and vice versa.

# Summary

In this chapter we looked at several ways of turning data structures into bytes on the
network or bytes on disk. We saw how the details of **these encodings affect not only
their efficiency, but more importantly also the architecture of applications and your
options for deploying them.**

- Rolling upgrades and Compatibility
  In particular, many services need to support rolling upgrades, where a new version of
  a service is gradually deployed to a few nodes at a time, rather than deploying to all
  nodes simultaneously. Rolling upgrades allow new versions of a service to be released
  without downtime (thus encouraging frequent small releases over rare big releases)
  and make deployments less risky (allowing faulty releases to be detected and rolled
  back before they affect a large number of users). These properties are hugely benefi‐
  cial for _evolvability_, the ease of making changes to an application.
  During rolling upgrades, or for various other reasons, we must assume that different
  nodes are running the different versions of our application’s code. Thus, it is impor‐
  tant that all data flowing around the system is encoded in a way that provides back‐
  ward compatibility (new code can read old data) and forward compatibility (old code
  can read new data).

We discussed several data encoding formats and their compatibility properties:

- **Programming language–specific encodings:** restricted to a single programming language and often fail to provide forward and backward compatibility.
- **Textual formats like JSON, XML, and CSV:** widespread, and their compatibility depends on how you use them. They have optional schema languages, which are sometimes helpful and sometimes a hindrance. These formats are somewhat vague about datatypes, so you have to be careful with things like numbers and binary strings.
- **Binary schema–driven formats:** like Thrift, Protocol Buffers, and Avro allow
  compact, efficient encoding with clearly defined forward and backward compatibility semantics. The schemas can be useful for documentation and code generation in statically typed languages. However, they have the downside that data
  needs to be decoded before it is human-readable.

We also discussed several modes of dataflow, illustrating different scenarios in which data encodings are important:

- **Databases**, where the process writing to the database encodes the data and the
  process reading from the database decodes it
- **RPC and REST APIs**, where the client encodes a request, the server decodes the
  request and encodes a response, and the client finally decodes the response
- **Asynchronous message passing (using message brokers or actors),** where nodes
  communicate by sending each other messages that are encoded by the sender
  and decoded by the recipient
  We can conclude that with a bit of care, backward/forward compatibility and rolling
  upgrades are quite achievable. May your application’s evolution be rapid and your
  deployments be frequent.
